![GitHub Repo](https://img.shields.io/badge/Research-Paper-blue)
# **Cross-Lingual NLU: Mitigating Language-Specific Impact in Embeddings Leveraging Adversarial Learning**
## ðŸ“œ Abstract
<p align="justify">
Low-resource languages and computational expenses pose significant challenges in the domain of large language models (LLMs). Currently, researchers are actively involved in various efforts to tackle these challenges. Cross-lingual natural language processing (NLP) remains one of the most promising strategies to address these issues. In this paper, we introduce a novel approach that utilizes adversarial techniques to mitigate the impact of language-specific information in contextual embeddings generated by large multilingual language models, with potential applications in cross-lingual tasks. The study encompasses five different languages, including both Latin and non-Latin ones, in the context of two fundamental tasks in natural language understanding: intent detection and slot filling. The results primarily show that our current approach excels in zero-shot scenarios for Latin languages like Spanish. However, it encounters limitations when applied to languages distant from English, such as Thai and Persian. This highlights that while our approach effectively reduces the effect of language-specific information on the core meaning, it performs better for Latin languages that share language-specific nuances with English, as certain characteristics persist in the overall meaning within embeddings.
</p>

## ðŸ“Š Results
<p align="justify">
The table below presents a <strong>sample of our results</strong> for the <strong>Spanish</strong> language in a <strong>zero-shot scenario</strong>, reporting <strong>accuracy</strong> and <strong>F1 score</strong>.  
These results highlight the significant superiority of the <strong>proposed method</strong> over <strong>baseline approaches</strong>.  
The table shows the mean values of the micro-average across five runs, demonstrating that the variance values approach zero, which indicates the model's high stability.
</p>

|Model                                  |ID (Acc.)   |SF (F1)  |
|----------------------------------------|--------|---------|
| CL. XLU embd.                          |36.94  |17.50   |
| CL. CoVe                               |37.13  |5.35    |
| CL. multi CoVe                         |53.34  |22.50   |
| CL. multi CoVe w/ auto                 |53.89  |19.25   |
| Zero-shot SLU                          |46.64  |15.41   |
| **Ours (variance)**                     |**68.74â€  (7e-5)** |**44.45â€  (2e-3)** |

*Results for Spanish on the [Facebook multilingual dataset](https://aclanthology.org/N19-1380/) utilizing Spanish auxiliary data.*  
â€ : Significant results with p-value < 1-e5.  

## ðŸ“Œ Citation

If you use this work, please cite our [paper](https://aclanthology.org/2024.lrec-main.370.pdf) as follows:

```bibtex
@article{Cross-Lingual_NLU2024,
  author    = {Saedeh Tahery and Sahar Kianian and Saeed Farzi},
  title     = {Cross-Lingual NLU: Mitigating Language-Specific Impact in Embeddings Leveraging Adversarial Learning},
  conference = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  year      = {2024},
  url       = {https://aclanthology.org/2024.lrec-main.370/}
}
```
